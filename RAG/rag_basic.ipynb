{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  Retrieval-Augmented Generation (RAG) with Shivaay API\n",
        "\n",
        "This notebook demonstrates a simple Retrieval-Augmented Generation (RAG) pipeline using the **Shivaay LLM API**.\n",
        "\n",
        "##  Goal:\n",
        "Answer complex user questions more accurately by:\n",
        "1. Generating **optimized search queries** from a user's question.\n",
        "2. Retrieving relevant **contextual documents** (mocked here).\n",
        "3. Asking the LLM to **answer using only that context**, reducing hallucinations.\n",
        "\n",
        "##  Pipeline Steps:\n",
        "1. **Generate Search Queries** – Get smarter queries to search for info.\n",
        "2. **Retrieve Context** – (Mocked) In practice, pull data using a vector database or web search.\n",
        "3. **Generate Answer with RAG** – Ask Shivaay to answer using only the given context.\n",
        "\n",
        ">  Make sure to replace `\"<YOUR-API-KEY>\"` with your actual API key.\n",
        "\n"
      ],
      "metadata": {
        "id": "iK2SGedQmGmv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 1: Import Required Library & Configure Shivaay API\n",
        "\n",
        "We start by importing `requests` to make API calls and configure our Shivaay API key and endpoint.\n"
      ],
      "metadata": {
        "id": "pw9UgRuKmMvv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "bBOBcl8pUb07"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "API_KEY = \"<YOUR-API-KEY>\"\n",
        "API_URL = \"https://api.futurixai.com/api/shivaay/v1/chat/completions\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 2: Generate Search Queries Using LLM\n",
        "\n",
        "This function sends a user query to Shivaay and asks it to generate 3 optimized search queries that can be used to retrieve relevant information. These act as better search terms for information retrieval.\n"
      ],
      "metadata": {
        "id": "kvki61e_mRjZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_search_queries(user_query):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"shivaay\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"Generate 3 optimized search queries to find relevant information for answering the user's question.\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_query\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.3,\n",
        "        \"max_tokens\": 150\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    queries = response.json()['choices'][0]['message']['content'].split('\\n')\n",
        "    return [q.strip('- ').strip() for q in queries if q.strip()]\n"
      ],
      "metadata": {
        "id": "3qN0wXtYlm9n"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 3: Generate Final Answer Using Retrieved Context\n",
        "\n",
        "This function sends both the user query and the retrieved document context to Shivaay, instructing it to answer based only on that context. If the context is not sufficient, it will respond with \"I don't have enough information\".\n"
      ],
      "metadata": {
        "id": "-MX3VXdSmVcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def answer_with_rag(user_query, context):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": f\"Bearer {API_KEY}\"\n",
        "    }\n",
        "\n",
        "    payload = {\n",
        "        \"model\": \"shivaay\",\n",
        "        \"messages\": [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": f\"\"\"Answer the question using only the provided context. If unsure, say \"I don't have enough information\".\n",
        "\n",
        "                Context:\n",
        "                {context}\"\"\"\n",
        "            },\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": user_query\n",
        "            }\n",
        "        ],\n",
        "        \"temperature\": 0.1,\n",
        "        \"max_tokens\": 200\n",
        "    }\n",
        "\n",
        "    response = requests.post(API_URL, headers=headers, json=payload)\n",
        "    return response.json()['choices'][0]['message']['content']\n"
      ],
      "metadata": {
        "id": "roA4TcQSltjT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Step 4: Example Usage\n",
        "\n",
        "We:\n",
        "1. Provide a sample user question\n",
        "2. Generate search queries\n",
        "3. Define mock context (replace with actual retrieved text in real use)\n",
        "4. Generate a final answer using that context\n",
        "\n",
        "This demonstrates the RAG workflow from query to answer.\n"
      ],
      "metadata": {
        "id": "220XPREKmaZQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "user_question = \"What are the most promising battery technologies for grid storage?\"\n",
        "search_queries = generate_search_queries(user_question)\n",
        "print(\"Generated search queries:\", search_queries)\n",
        "\n",
        "mock_context = \"\"\"\n",
        "1. Solid-state batteries show promise for grid storage with higher energy density.\n",
        "2. Flow batteries are gaining traction for large-scale applications due to scalability.\n",
        "3. Lithium-ion remains dominant but research continues into sodium-ion alternatives.\n",
        "\"\"\"\n",
        "\n",
        "final_answer = answer_with_rag(user_question, mock_context)\n",
        "print(\"\\nFinal Answer:\", final_answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rZP6DuwlwUB",
        "outputId": "dd20f849-3291-455a-af75-63be9041ece1"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated search queries: ['1. \"Most promising battery technologies for grid storage 2023\"', '2. \"Advancements in battery technology for large scale energy storage solutions\"', '3. \"Comparative analysis of emerging battery technologies for grid-scale applications\"']\n",
            "\n",
            "Final Answer: Based on the provided context, the most promising battery technologies for grid storage include solid-state batteries due to their higher energy density and flow batteries because of their scalability for large-scale applications. Additionally, lithium-ion batteries remain dominant, although there is ongoing research into sodium-ion alternatives as well.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1Soz-lTtl0p-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}